{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPqClxT4Scaf"
      },
      "source": [
        "# Employee Attrition Prediction using IBM HR Analytics Dataset\n",
        "This notebook walks through the full pipeline to build a predictive model for employee attrition and derive actionable HR insights using EDA, machine learning, and SHAP explainability."
      ],
      "id": "PPqClxT4Scaf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iw1GgYVScai"
      },
      "source": [
        "## Step 1: Load and Preprocess Data\n",
        "We start by importing the dataset and applying basic preprocessing steps including dropping constant/irrelevant columns, binary encoding, and one-hot encoding."
      ],
      "id": "1iw1GgYVScai"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNf8cythScaj"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/WA_Fn-UseC_-HR-Employee-Attrition.csv\")\n",
        "\n",
        "# Drop irrelevant columns\n",
        "df.drop(columns=[\"EmployeeNumber\", \"EmployeeCount\", \"Over18\", \"StandardHours\"], inplace=True)\n",
        "\n",
        "# Binary encoding\n",
        "df[\"Attrition\"] = df[\"Attrition\"].map({\"Yes\": 1, \"No\": 0})\n",
        "df[\"Gender\"] = df[\"Gender\"].map({\"Male\": 1, \"Female\": 0})\n",
        "df[\"OverTime\"] = df[\"OverTime\"].map({\"Yes\": 1, \"No\": 0})\n",
        "\n",
        "# One-hot encoding for categorical variables\n",
        "categorical_cols = [\"BusinessTravel\", \"Department\", \"EducationField\", \"JobRole\", \"MaritalStatus\"]\n",
        "df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n"
      ],
      "id": "TNf8cythScaj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1ZdsG5GScal"
      },
      "source": [
        "## Step 2: Exploratory Data Analysis (EDA)\n",
        "We analyze class distribution and correlations to understand key drivers of attrition."
      ],
      "id": "_1ZdsG5GScal"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCBJItCtScam"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Attrition class distribution\n",
        "sns.countplot(x=\"Attrition\", data=df)\n",
        "plt.title(\"Attrition Distribution\")\n",
        "plt.show()\n",
        "\n",
        "# Correlation with target\n",
        "corr = df.corr()[\"Attrition\"].sort_values(ascending=False)\n",
        "print(\"Top correlated features with Attrition:\\n\", corr.head(10))"
      ],
      "id": "TCBJItCtScam",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hu-9FyxvScam"
      },
      "source": [
        "## Step 3: Train-Test Split and Modeling\n",
        "We train Random Forest and Logistic Regression models after stratified train-test splitting."
      ],
      "id": "hu-9FyxvScam"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tagIw13XScan"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "X = df.drop(\"Attrition\", axis=1)\n",
        "y = df[\"Attrition\"]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Random Forest\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "print(\"Random Forest Classification Report:\")\n",
        "print(classification_report(y_test, rf_model.predict(X_test)))\n",
        "\n",
        "# Logistic Regression\n",
        "lr_model = LogisticRegression(max_iter=1000)\n",
        "lr_model.fit(X_train, y_train)\n",
        "print(\"Logistic Regression Classification Report:\")\n",
        "print(classification_report(y_test, lr_model.predict(X_test)))"
      ],
      "id": "tagIw13XScan",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wpkl4WvRScan"
      },
      "source": [
        "## Step 4: Model Explainability with SHAP\n",
        "We use SHAP to understand which features contribute most to predictions."
      ],
      "id": "Wpkl4WvRScan"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoQ_VuEuScao"
      },
      "source": [
        "import shap\n",
        "explainer = shap.Explainer(rf_model, X_test)\n",
        "shap_values = explainer(X_test)\n",
        "shap.summary_plot(shap_values, X_test)"
      ],
      "id": "FoQ_VuEuScao",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gs9hk8d7Scao"
      },
      "source": [
        "## Step 5: Actionable Insights\n",
        "Based on feature importance, here are actionable insights:\n",
        "- High overtime increases attrition risk → reduce workload or offer flexible shifts.\n",
        "- Low income correlates with attrition → consider salary adjustment.\n",
        "- Long commutes may lead to resignations → offer hybrid/remote options.\n",
        "- Young employees with low experience may leave → improve onboarding and career paths."
      ],
      "id": "gs9hk8d7Scao"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}